<div align="center">

# ğŸŒŒ BlackstarUI 

#### A simple user interface for inferencing LLMs.

Blackstar allows you to locally perform inference with large language models at various levels of quantization.
This project is compatible with the Transformers library and the HuggingFace hub. ğŸ¤—ğŸª„

![Screenshot](https://github.com/Celestinian/BlackstarUI/blob/main/screenshots/1.png?raw=true)

</div>

> [!IMPORTANT]
> You have the option to run models on either a CPU or a GPU. Your choice should depend on your hardware and the specific model's requirements.

# âš–ï¸ Screenshots 

You can check out some screenshots of the UI [here](examples.md).

# ğŸ¤“ FAQ 

- What does Blackstar do?
  - Blackstar employs a Python backend to offer a simplified, easy-to-use, and straightforward inference user interface for large language models.
- Is this project finished?
  - No, this project is a work in progress that I started during the summer. It is currently in the pre-alpha phase, and further updates are necessary before an official release.
- Do I need a GPU for this to work?
  - Not necessarily, as you have the option to switch between a CPU and a Cuda-Capable GPU in the model options. Please note that AMD cards are not supported at this time.
- How can I contribute to this project?
  - You can contribute by modifying the code, adding new features through PRs, reporting issues, or by providing recommendations. 

Not every model architecture is compatible with Blackstar. While most models in the HuggingFace Hub should work, this is not a universal rule.

> [!WARNING]
> This is an open-source project that is still in the early stages of development and is not yet ready for general use.

# â¤ Credits 

- Many of the application's icons were downloaded from [Icons8](https://icons8.com/). Extraction and reuse of Icons8 graphics is prohibited.

#

<div align="center">

</div>
